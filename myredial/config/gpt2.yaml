# basic configuration for building the model
tokenizer: 
    zh: uer/gpt2-chinese-cluecorpussmall
pretrained_model: 
    zh: uer/gpt2-chinese-cluecorpussmall
max_len: 128
min_len: 16 
topk: 50
topp: 1.0
temp: 1.0

# train configuration
train:
    lr: 0.00005
    grad_clip: 5.0
    seed: 0
    batch_size: 64
    max_len: 256
    epoch: 10
    warmup_ratio: 0.0
    checkpoint: 
        # path: bert-post/best_nspmlm.pt
        path: bert-fp/best_bert-base-chinese.pt
        is_load: false

# test configuration
test:
    seed: 0
    batch_size: 32
