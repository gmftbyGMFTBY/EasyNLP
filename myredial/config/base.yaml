datasets: 
    ecommerce: zh
    douban: zh
    writer: zh
    ubuntu: en

# pretrained model and tokenizer
tokenizer:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

pretrained_model:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

# the number of the gray negative samples (writer dataset)
gray_cand_num: 2

models:
    dual-bert: 
        type: Representation
        model_name: BERTDualEncoder
        dataset_name: BERTDualDataset
    dual-bert-gen: 
        type: Representation
        model_name: BERTSeq2SeqDualEncoder
        dataset_name: BERTDualDataset
    dual-bert-gray: 
        type: Representation
        model_name: BERTDualGrayEncoder
        dataset_name: BERTDualWithNegDataset
    dual-bert-hierarchical: 
        type: Representation
        model_name: BERTDualHierarchicalTrsEncoder
        dataset_name: BERTDualHierarchicalDataset
    bert-ft: 
        type: Interaction
        model_name: BERTRetrieval
        dataset_name: BERTWithNegDataset
    pj-bert-ft:
        type: Interaction
        model_name: PJBERTRetrieval
        dataset_name: BERTWithNegDataset
    sa-bert: 
        type: Interaction
        model_name: SABERTRetrieval 
        dataset_name: SABERTFTDataset
    poly-encoder: 
        type: LatentInteraction
        model_name: PolyEncoder
        dataset_name: BERTDualDataset

# huge file are saved in the root dir
root_dir: /apdcephfs/share_916081/johntianlan/MyReDial
# root_dir: /home/johntianlan/sources/MyReDial

# test interval: higher than 1 means that we donot test during training procedure, which is necessary for the corpus which has the big test set
test_interval: 1.1
# test_interval: 0.05
