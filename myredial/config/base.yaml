datasets: 
    ecommerce: zh
    douban: zh
    writer: zh
    lccc: zh
    ubuntu: en
    # inference dataset
    poetry: zh
    news_withsource: zh
    novel_selected: zh
    baike_summary: zh
    arxiv: en
    restoration-200k: zh

# pretrained model and tokenizer
tokenizer:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

pretrained_model:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

# the number of the gray negative samples (writer dataset)
gray_cand_num: 5
rank: null

no_test_models:
    - bert-fp

no_train_models:
    - bert-fp-original

models:
    dual-bert: 
        type: Representation
        model_name: BERTDualEncoder
        dataset_name: BERTDualDataset
        # inference_dataset_name: BERTDualInferenceDataset
        inference_dataset_name: BERTDualInferenceContextDataset
    dual-bert-ssl: 
        type: Representation
        model_name: BERTDualSSLEncoder
        dataset_name: BERTDualWithNegDataset
        inference_dataset_name: BERTDualInferenceDataset
        # inference_dataset_name: BERTDualInferenceContextDataset
    dual-bert-cl: 
        type: Representation
        model_name: BERTDualCLEncoder
        dataset_name: BERTDualCLDataset
        inference_dataset_name: BERTDualInferenceDataset
    dual-bert-fusion: 
        type: Representation
        model_name: BERTDualFusionEncoder
        dataset_name: BERTDualDataset
        inference_dataset_name: BERTDualCLInferenceDataset
    dual-bert-fusion-gray-writer: 
        type: Representation
        model_name: BERTDualFusionGrayFullEncoder
        dataset_name: BERTDualFullWithNegDataset
        inference_dataset_name: BERTDualCLInferenceDataset
    hash-bert: 
        type: Representation
        model_name: HashBERTDualEncoder
        dataset_name: BERTDualFullWithNegDataset
        # dataset_name: BERTDualDataset
        inference_dataset_name: BERTDualFullInferenceDataset
        # inference_dataset_name: BERTDualInferenceDataset
    hash-bert-poly: 
        type: Representation
        model_name: HashBERTPolyDualEncoder
        # dataset_name: BERTDualFullWithNegDataset
        dataset_name: BERTDualDataset
        inference_dataset_name: BERTDualInferenceDataset
    dual-bert-cb: 
        type: Representation
        model_name: BERTDualCBEncoder
        dataset_name: BERTDualDataset
        inference_dataset_name: BERTDualInferenceDataset
    dual-bert-gen: 
        type: Generation
        model_name: BERTSeq2SeqDualEncoder
        dataset_name: BERTDualDataset
        inference_dataset_name: null
    seq2seq: 
        type: Generation
        model_name: BERTSeq2SeqEncoder
        dataset_name: BERTDualDataset
        inference_dataset_name: null 
    dual-bert-gray: 
        type: Representation
        model_name: BERTDualGrayEncoder
        dataset_name: BERTDualFullWithNegDataset
        inference_dataset_name: BERTDualInferenceDataset
    dual-bert-gray-writer: 
        type: Representation
        model_name: BERTDualGrayFullEncoder
        # dataset_name: BERTDualFullWithNegDataset
        dataset_name: BERTDualArxivDataset
        # inference_dataset_name: BERTDualFullInferenceDataset
        inference_dataset_name: BERTDualFullWithSourceInferenceDataset
    dual-bert-hierarchical: 
        type: Representation
        model_name: BERTDualHierarchicalTrsEncoder
        dataset_name: BERTDualHierarchicalDataset
        inference_dataset_name: BERTDualInferenceDataset
    bert-ft: 
        type: Interaction
        model_name: BERTRetrieval
        dataset_name: BERTFTDataset
        inference_dataset_name: null 
    bert-fp-original: 
        type: Interaction
        model_name: BERTFPRetrieval
        dataset_name: BERTFTDataset
        inference_dataset_name: null 
    bert-ft-compare: 
        type: CompareInteraction
        model_name: BERTCompareRetrieval
        dataset_name: BERTFTCompDataset
        # BERTFTCompEvaluationDataset used for test.sh[compare mode]
        # dataset_name: BERTFTCompEvaluationDataset
        inference_dataset_name: null
    dual-bert-compare: 
        type: CompareInteraction
        model_name: BERTDualCompEncoder
        dataset_name: BERTDualWithNegDataset
        inference_dataset_name: null
    sa-bert: 
        type: Interaction
        model_name: SABERTRetrieval 
        dataset_name: SABERTFTDataset
        inference_dataset_name: null 
    poly-encoder: 
        type: LatentInteraction
        model_name: PolyEncoder
        dataset_name: BERTDualDataset
        inference_dataset_name: null
    bert-fp: 
        type: PostTrain
        model_name: BERTFPPostTrain
        dataset_name: PostTrainDataset
        inference_dataset_name: null 
    dual-bert-pt: 
        type: PostTrain 
        model_name: BERTDualPTEncoder
        dataset_name: BERTDualPTDataset
        # inference_dataset_name: BERTDualInferenceDataset
        inference_dataset_name: BERTDualInferenceContextDataset

# huge file are saved in the root dir
root_dir: /apdcephfs/share_916081/johntianlan/MyReDial
# root_dir: /home/johntianlan/sources/MyReDial

# test interval: higher than 1 means that we donot test during training procedure, which is necessary for the corpus which has the big test set
# test_interval: 1.1
test_interval: 0.1


# ========== deploy parameters ========== #
deploy:
    host: '0.0.0.0'
    port: 22335
    dataset: restoration-200k
    max_len: 256
    res_max_len: 64
    recall:
        activate: false
        model: bm25
        # default topk
        topk: 100
        # only the arxiv, writer dataset need the with_source
        with_source: false
        index_type: LSH 
        dimension: 768
    rerank:
        activate: false
        model: bert-ft
    pipeline:
        activate: true
