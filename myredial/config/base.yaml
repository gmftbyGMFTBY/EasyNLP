datasets: 
    ecommerce: zh
    douban: zh
    writer: zh
    ubuntu: en

# pretrained model and tokenizer
tokenizer:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

pretrained_model:
    # zh: hfl/chinese-roberta-wwm-ext
    zh: bert-base-chinese
    en: bert-base-uncased

# the number of the gray negative samples (writer dataset)
gray_cand_num: 2
xlm: false

models:
    dual-bert: 
        type: Representation
        model_name: BERTDualEncoder
        dataset_name: BERTDualDataset
    hash-bert: 
        type: Representation
        model_name: HashBERTDualEncoder
        dataset_name: BERTDualFullWithNegDataset
    xlm-dual-bert: 
        type: XLM
        model_name: XLMDualEncoder
        dataset_name: BERTDualDataset
    dual-bert-cb: 
        type: Representation
        model_name: BERTDualCBEncoder
        dataset_name: BERTDualDataset
    dual-bert-gen: 
        type: Generation
        model_name: BERTSeq2SeqDualEncoder
        dataset_name: BERTDualDataset
    seq2seq: 
        type: Generation
        model_name: BERTSeq2SeqEncoder
        dataset_name: BERTDualDataset
    dual-bert-gray: 
        type: Representation
        model_name: BERTDualGrayEncoder
        dataset_name: BERTDualFullWithNegDataset
    dual-bert-gray-writer: 
        type: Representation
        model_name: BERTDualGrayFullEncoder
        dataset_name: BERTDualFullWithNegDataset
    dual-bert-hierarchical: 
        type: Representation
        model_name: BERTDualHierarchicalTrsEncoder
        dataset_name: BERTDualHierarchicalDataset
    bert-ft: 
        type: Interaction
        model_name: BERTRetrieval
        dataset_name: BERTWithNegDataset
    sa-bert: 
        type: Interaction
        model_name: SABERTRetrieval 
        dataset_name: SABERTFTDataset
    poly-encoder: 
        type: LatentInteraction
        model_name: PolyEncoder
        dataset_name: BERTDualDataset

# huge file are saved in the root dir
root_dir: /apdcephfs/share_916081/johntianlan/MyReDial
# root_dir: /home/johntianlan/sources/MyReDial

# test interval: higher than 1 means that we donot test during training procedure, which is necessary for the corpus which has the big test set
test_interval: 1.1
# test_interval: 0.05


# ========== deploy parameters ========== #
deploy:
    host: '0.0.0.0'
    port: 22335
    dataset: writer
    recall:
        model: dual-bert-gray-full
        topk: 100
        index_type: IVF100,PQ16
        dimension: 768
    rerank:
        model: dual-bert
