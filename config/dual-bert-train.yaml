# train configuration
lr: 0.00005
grad_clip: 1.0
dropout: 0.1
test_interval: 0.05
smoothing: 0.1
seed: 50
batch_size: 32
max_len: 256
res_max_len: 64
epoch: 5
warmup_ratio: 0.1
# pretrained_model: bert-base-chinese
pretrained_model: hfl/chinese-roberta-wwm-ext
checkpoint: 
    path: best_nspmlm.pt
    is_load: false
