valid_during_training: false
is_step_for_training: false
temp: 0.05
doc_max_length: 256
dropout: 0.1
max_doc_num: 10
min_phrase_length: 3
max_phrase_length: 16
max_doc_length: 256
hard_neg_for_each_doc: 50
index_type: PCA64,IVF65536_HNSW32,PQ16
dimension: 1536
index_nprobe: 10

phrase_tokenizer: /apdcephfs/share_916081/johntianlan/bert-base-chinese
phrase_encoder_model: /apdcephfs/share_916081/johntianlan/bert-base-chinese
tokenizer: 
    zh: /apdcephfs/share_916081/johntianlan/gpt2-chinese-cluecorpussmall
    en: /apdcephfs/share_916081/johntianlan/gpt2_english
pretrained_model: 
    zh: /apdcephfs/share_916081/johntianlan/gpt2-chinese-cluecorpussmall
    en: /apdcephfs/share_916081/johntianlan/gpt2_english

# train configuration
train:
    load_param: true
    lr: 0.00005
    grad_clip: 1.0
    seed: 0
    batch_size: 8
    max_len: 512
    warmup_ratio: 0.
    epoch: 5
    iter_to_accumulate: 16
    checkpoint: 
        path: bert-post/best_nspmlm.pt
        is_load: false

# test configuration
test:
    seed: 0
    batch_size: 1
    prefix_length_rate: 0.5
    max_len: 256

inference:
    seed: 0
    batch_size: 256
    inf_phrase_min_len: 4
    inf_phrase_max_len: 32
    max_len: 512
    index_type: IVF262144_HNSW32,PQ16
    dimension: 1536
    index_nprobe: 5
